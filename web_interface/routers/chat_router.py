"""
èŠå¤©è·¯ç”±æ¨¡å—
"""
import time
import json
from typing import AsyncGenerator, Any
from SimpleLLMFunc import push_error
from fastapi import APIRouter, Request, Depends, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse

from ..models import (
    ChatCompletionRequest,
    Usage,
)
from context.conversation_manager import Conversation
from ..state import get_server_state, ServerState
from ..utils import (
    validate_chat_request,
    get_or_create_conversation,
    get_agent_for_model,
    process_agent_response,
    create_chat_response,
)
from ..error_handlers import create_error_response
from SimpleLLMFunc.logger import app_log, push_warning, log_context, get_current_context_attribute, get_location
from SimpleLLMFunc.llm_decorator.utils import extract_content_from_stream_response
from agent import BaseAgent

router = APIRouter(prefix="/v1/chat", tags=["chat"])


async def stream_chat_completion(
    request: ChatCompletionRequest,
    request_id: str,
    conversation: Conversation,
    agent: BaseAgent 
) -> AsyncGenerator[str, None]:
    """æµå¼èŠå¤©å®Œæˆç”Ÿæˆå™¨"""
    query, _ = validate_chat_request(request)

    app_log(f"ğŸ” Starting stream chat completion for conversation {conversation.uuid}, the query is {query}, agent is {agent.name}")

    created_time = int(time.time())

    # æµå¼å¤„ç†æ™ºèƒ½ä½“å“åº”
    content_buffer = ""
    try:
        with conversation:
            async for chunk in agent.run(query):
                # å°† raw åŒ…è½¬å‘ç»™å®¢æˆ·ç«¯
                try:
                    if hasattr(chunk, "model_dump_json"):
                        json_str = chunk.model_dump_json()
                    elif hasattr(chunk, "model_dump"):
                        json_str = json.dumps(chunk.model_dump(), ensure_ascii=False)
                    elif hasattr(chunk, "dict"):
                        json_str = json.dumps(chunk.dict(), ensure_ascii=False)
                    else:
                        json_str = json.dumps(chunk, default=str, ensure_ascii=False)
                except Exception as encode_err:
                    push_warning(f"Failed to encode raw chunk: {encode_err}")
                    json_str = json.dumps({"error": str(encode_err)}, ensure_ascii=False)

                # ä» raw åŒ…ä¸­æå–æ–‡æœ¬ï¼Œç”¨äºæœ€ç»ˆéæµå¼æ±‡æ€»å’ŒæŒä¹…åŒ–
                try:
                    content_delta = extract_content_from_stream_response(chunk, "agent_stream") or ""
                    if content_delta:
                        content_buffer += content_delta
                except Exception:
                    pass

                app_log(f"ğŸ” Forwarding raw chunk: {json_str}")
                yield f"data: {json_str}\n\n"
            
            # æµå¼å¯¹è¯å®Œæˆåç«‹å³æŒä¹…åŒ–conversation
            try:
                await conversation.context.persist()
                # ç›´æ¥è°ƒç”¨sketch_padçš„persistæ–¹æ³•
                conversation.sketch_pad.persist()
                app_log(f"âœ… Auto-saved conversation {conversation.uuid} after stream completion")
            except Exception as save_error:
                # ä¿å­˜å¤±è´¥ä¸åº”è¯¥å½±å“å“åº”ï¼Œä½†è¦è®°å½•æ—¥å¿—
                push_warning(f"âš ï¸ Warning: Failed to save conversation {conversation.uuid}: {save_error}")

    except Exception as e:
        err_obj: dict[str, Any] = {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": created_time,
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "delta": {"role": "assistant", "content": f"Error: {str(e)}"},
                    "finish_reason": "stop",
                }
            ],
        }
        err_json = json.dumps(err_obj, ensure_ascii=False)
        push_error(f"ğŸ” Sending error chunk: {err_json}", location=get_location())
        yield f"data: {err_json}\n\n"
        yield "data: [DONE]\n\n"
        return

    # ç»“æŸä¿¡å·
    yield "data: [DONE]\n\n"


@router.post(
    "/completions",
    dependencies=[Depends(get_server_state)],
    description="èŠå¤©å®Œæˆç«¯ç‚¹ï¼ˆç¬¦åˆOpenAIè§„èŒƒï¼‰",
)
async def chat_completions(
    request: ChatCompletionRequest,
    http_request: Request,
    state: ServerState = Depends(get_server_state),
):
    """èŠå¤©å®Œæˆç«¯ç‚¹ï¼ˆç¬¦åˆOpenAIè§„èŒƒï¼‰"""
    if not state.agent_registry:
        return create_error_response(
            message="Agent registry not initialized",
            error_type="server_error",
            status_code=500,
        )
    
    if not state.conversation_manager:
        return create_error_response(
            message="Conversation manager not initialized", 
            error_type="server_error",
            status_code=500,
        )

    # ä»è‡ªå®šä¹‰Headerè·å–conversation ID
    conversation_id = http_request.headers.get("X-Conversation-ID")


    with log_context(conversation_id=conversation_id):
    
        try:
            # è·å–æˆ–åˆ›å»ºconversation
            conversation, conversation_id = get_or_create_conversation(
                conversation_id, state.conversation_manager
            )
            
            # è·å–Agent
            agent = get_agent_for_model(request.model, state.agent_registry)
            
            # éªŒè¯è¯·æ±‚å¹¶è·å–å¿…è¦ä¿¡æ¯
            query, request_id = validate_chat_request(request)

            app_log(f"ğŸ” {request_id} request chat completion for conversation {conversation_id}")

            # æµå¼å“åº”
            if request.stream:
                return StreamingResponse(
                    stream_chat_completion(request, request_id, conversation, agent),
                    media_type="text/event-stream",
                    headers={
                        "Cache-Control": "no-cache",
                        "Connection": "keep-alive",
                        "Access-Control-Allow-Origin": "*",
                        "X-Conversation-ID": conversation_id,
                    },
                )

            # éæµå¼å“åº”ï¼šè¿”å›æ–‡æœ¬ + token ç»Ÿè®¡
            full_response, prompt_tokens, completion_tokens = await process_agent_response(query, conversation, agent)
            response = create_chat_response(request_id, request.model, full_response, prompt_tokens, completion_tokens)
            
            return JSONResponse(
                content=response.model_dump(),
                headers={"X-Conversation-ID": conversation_id}
            )

        except HTTPException:
            # é‡æ–°æŠ›å‡ºHTTPExceptionï¼Œè®©FastAPIå¤„ç†
            raise
        except Exception as e:
            return create_error_response(
                message=f"Internal server error: {str(e)}",
                error_type="server_error",
                status_code=500,
            )
